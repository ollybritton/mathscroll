{"title":"Inter-rater reliability","summary":"In statistics, inter-rater reliability (also called by various similar names, such as inter-rater agreement, inter-rater concordance, inter-observer reliability, inter-coder reliability, and so on) is the degree of agreement among independent observers who rate, code, or assess the same phenomenon.","image":"Comparison_of_rubrics_for_evaluating_inter-rater_kappa_(and_intra-class_correlation)_coefficients.png.webp","url":"Inter-rater_reliability"}