{"title":"Kullback–Leibler divergence","summary":"In mathematical statistics, the  Kullback–Leibler divergence (also called relative entropy and I-divergence[1]), denoted D KL ( P ∥ Q ) {\\displaystyle D_{\\text{KL}}(P\\parallel Q)} , is a type of statistical distance: a measure of how one probability distribution P is different from a second, reference probability distribution Q.[2][3] A simple interpretation of the KL divergence of P from Q is the expected excess surprise from using Q as a model when the actual distribution is P. While it is a distance, it is not a metric, the most familiar type of distance: it is not symmetric in the two distributions (in contrast to variation of information), and does not satisfy the triangle inequality. Instead, in terms of information geometry, it is a type of divergence,[4] a generalization of squared distance, and for certain classes of distributions (notably an exponential family), it satisfies a generalized Pythagorean theorem (which applies to squared distances).[5]","image":"039fa82bd08654b4faa2b32ded70c0160554fa07.svg","url":"Kullback–Leibler_divergence"}