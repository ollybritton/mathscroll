{"title":"Loss functions for classification","summary":"In machine learning and mathematical optimization, loss functions for classification are computationally feasible loss functions representing the price paid for inaccuracy of predictions in classification problems (problems of identifying which category a particular observation belongs to).[1]  Given X {\\displaystyle {\\mathcal {X}}} as the space of all possible inputs (usually X ⊂ R d {\\displaystyle {\\mathcal {X}}\\subset \\mathbb {R} ^{d}} ), and Y = { − 1 , 1 } {\\displaystyle {\\mathcal {Y}}=\\{-1,1\\}} as the set of labels (possible outputs), a typical goal of classification algorithms is to find a function f : X → Y {\\displaystyle f:{\\mathcal {X}}\\to {\\mathcal {Y}}} which best predicts a label y {\\displaystyle y} for a given input x → {\\displaystyle {\\vec {x}}} .[2]  However, because of incomplete information, noise in the measurement, or probabilistic components in the underlying process, it is possible for the same x → {\\displaystyle {\\vec {x}}} to generate different y {\\displaystyle y} .[3]  As a result, the goal of the learning problem is to minimize expected loss (also known as the risk), defined as","image":"8c7e5461c5286852df4ef652fca7e4b0b63030e9.svg","url":"Loss_functions_for_classification"}