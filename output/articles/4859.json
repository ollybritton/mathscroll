{"title":"Entropy (information theory)","summary":"In information theory, the entropy of a random variable is the average level of \"information\", \"surprise\", or \"uncertainty\" inherent to the variable's possible outcomes. Given a discrete random variable X {\\displaystyle X} , which takes values in the alphabet X {\\displaystyle {\\mathcal {X}}} and is distributed according to p : X â†’ [ 0 , 1 ] {\\displaystyle p:{\\mathcal {X}}\\to [0,1]} :","image":"68baa052181f707c662844a465bfeeb135e82bab.svg","url":"Entropy_(information_theory)"}