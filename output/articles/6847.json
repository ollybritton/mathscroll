{"title":"Hamilton–Jacobi–Bellman equation","summary":"In optimal control theory, the Hamilton-Jacobi-Bellman (HJB) equation gives a necessary and sufficient condition for optimality of a control with respect to a loss function.[1] It is, in general, a nonlinear partial differential equation in the value function, which means its solution is the value function itself. Once this solution is known, it can be used to obtain the optimal control by taking the maximizer (or minimizer) of the Hamiltonian involved in the HJB equation.[2][3]","image":"35ccef2d3dc751e081375d51c111709d8a1d7ac6.svg","url":"Hamilton–Jacobi–Bellman_equation"}