{"title":"Jensen–Shannon divergence","summary":"In probability theory and statistics, the Jensen–Shannon divergence is a method of measuring the similarity between two probability distributions.  It is also known as information radius (IRad)[1] [2] or total divergence to the average.[3] It is based on the Kullback–Leibler divergence, with some notable (and useful) differences, including that it is symmetric and it always has a finite value. The square root of the Jensen–Shannon divergence is a metric often referred to as Jensen–Shannon distance.[4][5][6]","image":"94ac623f456d31f2b21cae93e88314c991c64085.svg","url":"Jensen–Shannon_divergence"}