{"title":"Cross entropy","summary":"In information theory, the cross-entropy between two probability distributions p {\\displaystyle p} and q {\\displaystyle q} over the same underlying set of events measures the average number of bits needed to identify an event drawn from the set if a coding scheme used for the set is optimized for an estimated probability distribution q {\\displaystyle q} , rather than the true distribution p {\\displaystyle p} .","image":"81eac1e205430d1f40810df36a0edffdc367af36.svg","url":"Cross_entropy"}