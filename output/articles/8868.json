{"title":"Kullback's inequality","summary":"In information theory and statistics, Kullback's inequality is a lower bound on the Kullback–Leibler divergence expressed in terms of the large deviations rate function.[1]  If P and Q are probability distributions on the real line, such that P is absolutely continuous with respect to Q, i.e. P << Q, and whose first moments exist, then","image":"3b7580b07729495a34c806ba6c967da160825976.svg","url":"Kullback's_inequality"}