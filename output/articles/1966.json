{"title":"Bretagnolle–Huber inequality","summary":"In information theory, the Bretagnolle–Huber inequality bounds the total variation distance between two probability distributions P {\\displaystyle P} and Q {\\displaystyle Q} by a concave and bounded function of the Kullback–Leibler divergence D K L ( P ∥ Q ) {\\displaystyle D_{\\mathrm {KL} }(P\\parallel Q)} . The bound can be viewed as an alternative to the well-known Pinsker's inequality: when D K L ( P ∥ Q ) {\\displaystyle D_{\\mathrm {KL} }(P\\parallel Q)} is large (larger than 2 for instance.[1]), Pinsker's inequality is vacuous, while Bretagnolle–Huber remains bounded and hence non-vacuous. It is used in statistics and machine learning to prove information-theoretic lower bounds relying on hypothesis testing[2]","image":"b4dc73bf40314945ff376bd363916a738548d40a.svg","url":"Bretagnolle–Huber_inequality"}